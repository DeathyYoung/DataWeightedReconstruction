This paper describes a method for linear text seg- mentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). 
Inter-sentence similarity is replaced by rank in the local context. 
Boundary locations are discovered by divisive clustering.
A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al. , 1996; Yaari, 1997; Kan et al. , 1998; Choi, 2000; Nakao, 2000).
Some evaluations circumvent this difficulty by detecting boundaries in sets of concatenated documents, where there can be no disagreements about the fact of the matter (Reynar 1994; Choi 2000); others have several human judges make ratings to produce a gold standard. The second difficulty with evaluating these algorithms is that for different applications of text segmentation, different kinds of errors become important.
We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in (Choi, 2000) on a heldout development set of three lectures.
The Natural Language Processing (NLP) community has come up with various solutions towards topic-based text segmentation (e.g. , Hearst, 1994; Choi, 2000; Malioutov and Barzilay, 2006).
Because the notion of a topic is inherently subjective, we follow many researchers who have reported results on "pseudo-documents"documents formed by concatenating several randomly selected documentsso that the boundaries of segments are known, sharp, and not dependent on annotator variability (Choi, 2000).
Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al. , 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al. , 1999).
We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000)
Most of them only rely on surface features of documents: word reiteration in (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Galley et al. , 2003) or discourse cues in (Passonneau and Litman, 1997; Galley et al. , 2003).
With news documents from Chinese websites, collected from 10 different categories, we design an artificial test corpus in the similar way of (Choi, 2000), in which we take each n-sentence document as a coherent topic segment, randomly choose ten such segments and concatenate them as a sample.
In (Choi 2000) boundaries are hypothesized using sentences as the basic unit of text; however both C99 and TextTiling can take advantage of paragraph information when the input consists of one paragraph per line.
Though related to the task of topic segmentation which stimulated a large number of studies (Hearst, 1997; Choi, 2000; Galley et al. , 2003, inter alia), paragraph segmentation has not been thoroughly investigated so far.
Previous research has analyzed various facets of lexical distribution, including lexical weighting, similarity computation, and smoothing (Hearst, 1994; Utiyama and Isahara, 2001; Choi, 2000; Reynar, 1998; Kehagias et al. , 2003; Ji and Zha, 2003).
