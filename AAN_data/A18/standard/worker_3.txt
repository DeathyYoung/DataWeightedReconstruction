This paper describes a method for linear text seg- mentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). 
As (Choi, 2000) or (Kaufmann, 1999), we use the cosine measure for evaluating the similarity between a vector of the context window (Vw) and the equivalent vector in the segment context (Vs): = i ix i ix ix i ix xx CwwwgCswwg CwwgwCswwg VwVssim 22 ),(),( ),(),( ),( (5) where wgx(wi,Cs,w}) is the weight of the word wi in the vector x (txt or coll) of the context Cs,w}.
In (Choi 2000) boundaries are hypothesized using sentences as the basic unit of text; however both C99 and TextTiling can take advantage of paragraph information when the input consists of one paragraph per line.
We follow Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately.
Previous research has analyzed various facets of lexical distribution, including lexical weighting, similarity computation, and smoothing (Hearst, 1994; Utiyama and Isahara, 2001; Choi, 2000; Reynar, 1998; Kehagias et al. , 2003; Ji and Zha, 2003).
Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006)).
The second uses the text segmentation software of Choi (2000) to partition the decision-related DAs (ordered according to time) into several topic-based groups (CHOISEGMENT).
Though related to the task of topic segmentation which stimulated a large number of studies (Hearst, 1997; Choi, 2000; Galley et al. , 2003, inter alia), paragraph segmentation has not been thoroughly investigated so far.
Improved version of DotPlotting algorithm called C99 (Choi, 2000) uses DotPlotting chart for visualization of similarity measurements at consecutive point of the text (thus resulting in point with different levels of intensity) instead of words cooccurences.
Most of them only rely on surface features of documents: word reiteration in (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Galley et al. , 2003) or discourse cues in (Passonneau and Litman, 1997; Galley et al. , 2003).
Because the notion of a topic is inherently subjective, we follow many researchers who have reported results on "pseudo-documents"documents formed by concatenating several randomly selected documentsso that the boundaries of segments are known, sharp, and not dependent on annotator variability (Choi, 2000).
Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts: word distribution, as in (Hearst, 1997), (Choi, 2000) and (Utiyama and Isahara, 2001), or linguistic cues as in (Passonneau and Litman, 1997).
Other techniques use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994; Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue words, or hand-selected cue words to detect segment boundaries (Passonneau and Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998).
While other relations between lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001).
The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., Choi 2000; Hearst 1997; Heinonen 1998; Kehagias, Pavlina, and Petridis 2003; Utiyama and Isahara 2001), and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., Kozima 1993; Lin et al. 2004; Morris and Hirst 1991), or from collocations collected in large corpora (Bolshakov and Gelbukh 2001; Brants, Chen, and Tsochantaridis 2002; Choi et al. 2001; Ferret 2002; Kaufmann 1999; Ponte and Croft 1997).
