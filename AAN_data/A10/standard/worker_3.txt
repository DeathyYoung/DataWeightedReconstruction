The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004)
The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004)
We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes
Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004)
We describe a highly ef-ficient monotone search algorithm with a com-plexity linear in the input sentence length
Above the phrase level, some models perform no reordering (Zens and Ney 2004; Kumar, Deng, and Byrne 2006), some have a simple distortion model that reorders phrases independently of their content (Koehn, Och, and Marcu 2003; Och and Ney 2004), and some, for example, the Alignment Template System (Och et al
The so-called phrase-based and N-gram-based models are two examples of these approaches (Zens and Ney, 2004; Marino et al
Therefore, we search for the best pronunciation over all segmentations of the word, adapting the monotone search algorithm proposed by Zens and Ney (2004) for phrase-based machine translation
Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al
There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004)
The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004)
In particular, our methods are inspired by the monotone search algorithm proposed in (Zens and Ney, 2004)
, 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model
Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table
