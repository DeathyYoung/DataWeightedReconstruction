When this analysis is coupled with the projective parsing algorithms of Eisner (1996) andPaskin (2001) we beginto get aclear picture of the complexity for data-driven dependency parsing within an edge-factored framework
The optimal parse can be found using a spanning tree algorithm (Eisner, 1996; McDonald et al
It combines online Peceptron learning (Collins, 2002) with a parsing model based on the Eisner algorithm (Eisner, 1996), extended so as to jointly assign syntactic and semantic labels
Dependency-based statistical language modeling and analysis have also become quite popular in statistical natural language processing (Lafferty et al
, 2008), while in the case of general non-projective dependency forests, it is only tractable under strong independence assumptions (McDonald et al
Like context-free grammars, projective dependency trees are not sufficient to represent all the linguistic phenomena observed in natural languages, but they have the advantage of being efficiently parsable: their parsing problem can be solved in cubic time with chart parsing techniques (Eisner, 1996; Gomez-Rodrguez et al
The DP algorithms are generally variants of the CKY bottom-up chart parsing algorithm such as that proposed by Eisner (1996)
, 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996)
Nivre and McDonald (2008) instead use hints from one parse as features in a second parse, exploiting the complementary properties of graph-based parsers (Eisner, 1996; McDonald et al
2 On re-parsing algorithms To guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of Eisner (1996) for reparsing (Sagae and Lavie, 2006; Hall et al
Many probabilistic evaluation models have been published inspired by one or more of these feature types [Black, 1992] [Briscoe, 1993] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996], but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively
While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding
Following the edge based factorization method of Eisner (1996), the score of a dependency tree in the first order version is defined as the sum of the scores of all edges in the tree
Parsing algorithms which search the entire space (Eisner, 1996; McDonald, 2006) are restricted in the features they use to score a relation
We employ the second-order projective graphbased parsing model of Carreras (2007), which is an extensionof the projective parsing algorithm of Eisner (1996)
