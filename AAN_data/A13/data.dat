Title:P05-1012		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:McDonald, Ryan; Crammer, Koby; Pereira, Fernando
We present an effective training al-gorithm for linearly-scored dependencyparsers that implements online large-margin multi-class training (Crammer andSinger, 2003; Crammer et al, 2003) ontop of efficient parsing techniques for de-pendency trees (Eisner, 1996). The trainedparsers achieve a competitive dependencyaccuracy for both English and Czech withno language specific enhancements.

Title:W05-1506		Conference:International Workshop On Parsing Technology		Author:Huang, Liang; Chiang, David
Although generative models, being probabilitybased, do not suffer from this problem, more general models (e.g. , log-linear models) may require negative edge costs (McDonald et al. , 2005; Taskar et al. , 2004).

Title:H05-1066		Conference:Human Language Technology Conference And Empirical Methods In Natural Language Processing		Author:McDonald, Ryan; Pereira, Fernando; Ribarov, Kiril; Haji&#x10D;, Jan
To learn these structures we used online large-margin learning (McDonald et al. , 2005) that empirically provides state-of-the-art performance for Czech. (self citation)

Title:H05-1066		Conference:Human Language Technology Conference And Empirical Methods In Natural Language Processing		Author:McDonald, Ryan; Pereira, Fernando; Ribarov, Kiril; Haji&#x10D;, Jan
We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al. , 2003; McDonald et al. , 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies. (self citation)

Title:H05-1066		Conference:Human Language Technology Conference And Empirical Methods In Natural Language Processing		Author:McDonald, Ryan; Pereira, Fernando; Ribarov, Kiril; Haji&#x10D;, Jan
This algorithm has a runtime of O(n3) and has been employed successfully in both generative and discriminative parsing models (Eisner, 1996; McDonald et al. , 2005). (self citation)

Title:P05-1067		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Ding, Yuan; Palmer, Martha
The recent advances in parsing have achieved parsers with 3 ()On time complexity without the grammar constant (McDonald et al. , 2005).

Title:P06-2009		Conference:International Conference On Computational Linguistics And Annual Meeting Of The Association For Computational Linguistics - Poster Sessions		Author:Chang, Ming-Wei; Do, Quang; Roth, Dan
(McDonald et al. , 2005) build on this work, and use a global discriminative training approach to improve the edges scores, along with Eisners algorithm, to yield the expected improvement.

Title:P06-2009		Conference:International Conference On Computational Linguistics And Annual Meeting Of The Association For Computational Linguistics - Poster Sessions		Author:Chang, Ming-Wei; Do, Quang; Roth, Dan
4.2 Evaluation We use the same evaluation metrics as in (McDonald et al. , 2005).

Title:P06-2009		Conference:International Conference On Computational Linguistics And Annual Meeting Of The Association For Computational Linguistics - Poster Sessions		Author:Chang, Ming-Wei; Do, Quang; Roth, Dan
When evaluating the result, we exclude the punctuation marks, as done in (McDonald et al. , 2005) and (Yamada and Matsumoto, 2003).

Title:P06-2009		Conference:International Conference On Computational Linguistics And Annual Meeting Of The Association For Computational Linguistics - Poster Sessions		Author:Chang, Ming-Wei; Do, Quang; Roth, Dan
Dependency structures are more ef cient to parse (Eisner, 1996) and are believed to be easier to learn, yet they still capture much of the predicate-argument information needed in applications (Haghighi et al. , 2005), which is one reason for the recent interest in learning these structures (Eisner, 1996; McDonald et al. , 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004).

Title:W06-2925		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Carreras, Xavier; Surdeanu, Mihai; M&agrave;rquez, Llu&iacute;s
The features used to score, while based on the previous work in dependency parsing (McDonald et al. , 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed.

Title:W06-2925		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Carreras, Xavier; Surdeanu, Mihai; M&agrave;rquez, Llu&iacute;s
Most of these features are inspired by previous work in dependency parsing (McDonald et al. , 2005; Collins, 1999).

Title:W06-2925		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Carreras, Xavier; Surdeanu, Mihai; M&agrave;rquez, Llu&iacute;s
partial parsing (Carreras et al. , 2005) or even dependency parsing (McDonald et al. , 2005).

Title:P06-2041		Conference:International Conference On Computational Linguistics And Annual Meeting Of The Association For Computational Linguistics - Poster Sessions		Author:Hall, Johan; Nivre, Joakim; Nilsson, Jens
Alternatively, discriminative models can be used to search the complete space of possible parses (Taskar et al. , 2004; McDonald et al. , 2005).

Title:W06-1615		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Blitzer, John; McDonald, Ryan; Pereira, Fernando
MIRA has been used successfully for both sequence analysis (McDonald et al. , 2005a) and dependency parsing (McDonald et al. , 2005b). (self citation)

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
From the fact that neither MIRA nor BPM clearly outperforms the other, we conclude that we have successfully replicated the results reported in (McDonald et al. , 2005a) for English.

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
For the comparison of English dependency accuracy excluding punctuation, MIRA and BPM are both statistically significantly better than the averaged perceptron result reported in (McDonald et al. , 2005a).

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
The results reported here for English and Czech are comparable to the previous best published numbers in (McDonald et al. , 2005a), as Table 3 shows.

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
2 Refer to (McDonald et al. , 2005b) for a detailed treatment of both algorithms.

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
Again, since neither MIRA nor BPM outperforms the other on all measures, we conclude that the results constitute a valiation of the results reported in (McDonald et al. , 2005a).

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
Examples include the margin perceptron (Duda et al. , 2001), ALMA (Gentile, 2001), and MIRA (which is used to train the parser in (McDonald et al. , 2005a)).

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
The feature types are essentially those described in (McDonald et al. , 2005a).

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
1 3 Parser Architecture We take as our starting point a re-implementation of McDonalds state-of-the-art dependency parser (McDonald et al. , 2005a).

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
7 Conclusions We have successfully replicated the state-of-the-art results for dependency parsing (McDonald et al. , 2005a) for both Czech and English, using Bayes Point Machines.

Title:N06-1021		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics		Author:Corston-Oliver, Simon H.; Aue, Anthony; Duh, Kevin; Ringger, Eric K.
Perceptron 82.9 88.0 30.3 (inc punc) MIRA 83.3 88.6 31.3 Bayes Point Machine 84.0 88.8 30.9 Table 3: Comparison to previous best published results reported in (McDonald et al. , 2005a).

Title:E06-1010		Conference:Conference Of The European Association For Computational Linguistics		Author:Nivre, Joakim
Moreover, the study of formal grammarsisonly partially relevant for research ondatadriven dependency parsing, where most systems are not grammar-based but rely on inductive inference from treebank data (Yamada and Matsumoto, 2003; Nivre et al. , 2004; McDonald et al. , 2005a).

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
For standard scoring functions, parsing requires an a72a58a4a6a73a75a74a12a17 dynamic programming algorithm to compute a projective tree that obtains the maximum score (Eisner and Satta, 1999; Wang et al. , 2005; McDonald et al. , 2005).

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Collins, 1997; Charniak, 2000; Wang et al. , 2005; McDonald et al. , 2005).

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
To train a77 we follow the large margin training approach of (Taskar et al. , 2003; Tsochantaridis et al. , 2004), which has been applied with great success to dependency parsing (Taskar et al. , 2004; McDonald et al. , 2005).

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
Large margin training can be expressed as minimizing a regularized loss (Hastie et al. , 2004) a45a58a113a115a114 a77 a116a117 a77 a78 a77 a118 (3) a59 a20 a45a47a39a26a48 a119a67a62a121a120 a4a123a122a25a20a124a9 a19 a20a100a17a102a101a125a4 sa4a68a77a75a9 a19 a20a100a17a126a101 sa4a68a77a75a9a44a122a25a20a100a17a14a17 where a19 a20 is the target tree for sentence a2 a20 ; a122 a20 ranges over all possible alternative trees in a31a32a4a33a2a127a20a33a17 ; sa4a68a77a102a9a19 a17a128a3 a129 a53a61a60a63a62a95a64a32a60a67a66a130a57a68a50a29a49 a77a54a78a102a79a67a4a6a5a16a20a105a21 a5a25a24a71a17 ; and a120 a4a123a122a25a20a124a9 a19 a20a100a17 is a measure of distance between the two trees a122a25a20 and a19 a20 . Using the techniques of (Hastie et al. , 2004) one can show that minimizing (4) is equivalent to solving the quadratic program a45a47a113a115a114 a131a71a132a133 a116a117 a77a54a78a102a77a134a118a98a135a136a78a75a137 subject to (4) a138 a20a102a139 a120 a4 a19 a20a97a9a44a122a140a20a100a17a103a118 sa4a68a77a75a9a44a122a25a20a68a17a126a101 sa4a68a77a102a9 a19 a20a33a17 for all a141a44a9a44a122a25a20 a35 a31a32a4a33a2a142a20a68a17 which corresponds to the training problem posed in (McDonald et al. , 2005).

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
Currently, the work on conditional parsing models appears to have culminated in large margin training (Taskar et al. , 2003; Taskar et al. , 2004; Tsochantaridis et al. , 2004; McDonald et al. , 2005), which currently demonstrates the state of the art performance in English dependency parsing (McDonald et al. , 2005).

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
Although (McDonald et al. , 2005) explicitly describes this as an advantage over previous approaches (Ratnaparkhi, 1999; Yamada and Matsumoto, 2003), below we nd that changing the loss to enforce a more detailed set of constraints leads to a more effective approach.

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
It turns out that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach the structured margin loss (Taskar et al. , 2003; Tsochantaridis et al. , 2004; McDonald et al. , 2005) is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component.

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al. , 2005), a suf ciently uni ed view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches.

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
First, there are exponentially many constraints corresponding to each possible parse of each training sentence which forces one to use alternative training procedures, such as incremental constraint generation, to slowly converge to a solution (McDonald et al. , 2005; Tsochantaridis et al. , 2004).

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
In which case, the parsing problem reduces to a19a38a37 a3a40a39a42a41a44a43a46a45a47a39a26a48 a49a51a50a42a52a54a53a56a55a58a57 a59 a53a61a60a63a62a6a64a65a60a67a66a44a57a68a50a29a49 sa4a6a5 a20 a21a69a5 a24 a17 (1) where the score sa4a6a5a27a20a70a21 a5a25a24a71a17 can depend on any measurable property of a5a30a20 and a5a25a24 within the tree a19 . This formulation is suf ciently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al. , 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al. , 2005).

Title:W06-2904		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wang, Qin Iris; Cherry, Colin; Lizotte, Dan; Schuurmans, Dale
Dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc a4a6a5a27a20a25a21a28a5a25a24a29a17, then a5a16a20 is an ancestor of all the words between a5a30a20 and a5a25a24 . Let a31a32a4a33a2a34a17 denote the set of all the directed, projective trees that span a2 . Given an input sentence a2, we would like to be able to compute the best parse; that is, a projective tree, a19a36a35 a31a32a4a33a2a34a17, that obtains the highest score . In particular, we follow (Eisner, 1996; Eisner and Satta, 1999; McDonald et al. , 2005) and assume that the score of a complete spanning tree a19 for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair).

Title:N06-3004		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics - Doctoral Consortium		Author:Huang, Liang
This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al. , 2005).

Title:N06-3004		Conference:Human Language Technology Conference And Meeting Of The North American Association For Computational Linguistics - Doctoral Consortium		Author:Huang, Liang
These algorithms have been re-implemented by other researchers in the field, including Eugene Charniak for his n-best parser, Ryan McDonald for his dependency parser (McDonald et al. , 2005), Microsoft Research NLP group (Simon Corston-Oliver and Kevin Duh, p.c).

Title:P06-1091		Conference:International Conference On Computational Linguistics And Annual Meeting Of The Association For Computational Linguistics		Author:Tillmann, Christoph; Zhang, Tong
Moreover, under this view, SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing, and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task, e.g. the on-line training algorithm presented in (McDonald et al. , 2005) and the perceptron training algorithm presented in (Collins, 2002).

Title:W06-1616		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Riedel, Sebastian; Clarke, James
However, global constraints cannot be incorporated into the CLE algorithm (McDonald et al. , 2005b).

Title:W06-1616		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Riedel, Sebastian; Clarke, James
The constraints are chosen based on the two criteria: (1) adding them to the base constraints (those added in advance) would result in an extremely large program, and (2) it must be efcient to detect whether the constraint is violated in y. No Cycles (T2) For every possible cycle c for the sentence x we have a constraint which forbids the case where all edges in c are active simultaneously: summationdisplay (i,j)c di,j |c|1 Comma Coordination (C3) For each symmetric conjunction token i which forms a symmetric coordination and each set of tokens A in x to the left of i with no comma between each pair of successive tokens we add: summationdisplay aA di,a |A|1 which forbids con gurations where i has the argument tokens A. Compatible Coordination Arguments (C4) For each conjunction token i and each set of tokens A in x with incompatible POS tags, we add a constraint to forbid con gurations where i has the argument tokens A. summationdisplay aA di,a |A|1 Selective Projective Parsing (P1) For each pair of triplets (i,j,l1) and (m,n,l2) we add the constraint: ei,j,l1 + em,n,l2 1 if l1 or l2 is in P. 3.2 Training For training we use single-best MIRA (McDonald et al. , 2005a).

Title:W06-1616		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Riedel, Sebastian; Clarke, James
The verb krijg is incorrectly coordinated with the preposition om . work is ef cient and has also been extended to non-projective trees (McDonald et al. , 2005b).

Title:W06-1616		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Riedel, Sebastian; Clarke, James
While we expect a longer runtime than using the Chu-Liu-Edmonds as in previous work (McDonald et al. , 2005b), we are interested in how large the increase is. Table 2 shows the average solve time (ST) for sentences with respect to the number of tokens in each sentence for our system with constraints (cnstr) and the Chu-Liu-Edmonds (CLE) algorithm.

Title:W06-1616		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Riedel, Sebastian; Clarke, James
This allows us to ef ciently use ILP for dependency parsing and add constraints which provide a signi cant improvement over the current stateof-the-art parser (McDonald et al. , 2005b) on the Dutch Alpino corpus (see bl row in Table 1).

Title:W06-2928		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Corston-Oliver, Simon H.; Aue, Anthony
As described in (Corston-Oliver et al. , 2006), we reimplemented the parser described in (McDonald et al. , 2005) and validated their results for Czech and English.

Title:W06-2928		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Corston-Oliver, Simon H.; Aue, Anthony
Additional features are created by combining these atomic features, as described in (McDonald et al. , 2005).

Title:W06-2932		Conference:Conference On Computational Natural Language Learning CoNLL		Author:McDonald, Ryan; Lerman, Kevin; Pereira, Fernando
These results show that the discriminative spanning tree parsing framework (McDonald et al. , 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages. (self citation)

Title:W06-2932		Conference:Conference On Computational Natural Language Learning CoNLL		Author:McDonald, Ryan; Lerman, Kevin; Pereira, Fernando
We use the MIRA 217 online learner to set the weights (Crammer and Singer, 2003; McDonald et al. , 2005a) since we found it trained quickly and provide good performance. (self citation)

Title:W06-2932		Conference:Conference On Computational Natural Language Learning CoNLL		Author:McDonald, Ryan; Lerman, Kevin; Pereira, Fernando
(McDonald et al. , 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English. (self citation)

Title:W06-2932		Conference:Conference On Computational Natural Language Learning CoNLL		Author:McDonald, Ryan; Lerman, Kevin; Pereira, Fernando
That work extends the maximum spanning tree dependency parsing framework (McDonald et al. , 2005a; McDonald et al. , 2005b) to incorporate features over multiple edges in the dependency graph. (self citation)

Title:W06-2920		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Buchholz, Sabine; Marsi, Erwin
(2005), while McDonalds parser has been applied to English (McDonald et al. , 2005a), Czech (McDonald et al. , 2005b) and, very recently, Danish (McDonald and Pereira, 2006).

Title:W06-2920		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Buchholz, Sabine; Marsi, Erwin
The search for the best parse can then be formalized as the search for the maximum spanning tree (MST) (McDonald et al. , 2005b).

Title:P06-2098		Conference:International Conference On Computational Linguistics And Annual Meeting Of The Association For Computational Linguistics - Poster Sessions		Author:Shimizu, Nobuyuki; Haas, Andrew R.
In particuler, Single-best MIRA (McDonald et al, 2005) uses only the single margin constraint for the runner up y with the highest score.

Title:P06-2098		Conference:International Conference On Computational Linguistics And Annual Meeting Of The Association For Computational Linguistics - Poster Sessions		Author:Shimizu, Nobuyuki; Haas, Andrew R.
(McDonald et al, 2005) presents this approach for dependency parsing.

Title:E06-1038		Conference:Conference Of The European Association For Computational Linguistics		Author:McDonald, Ryan
To do this we parse every sentence twice, once with a dependency parser (McDonald et al. , 2005b) and once with a phrase-structure parser (Charniak, 2000). (self citation)

Title:E06-1038		Conference:Conference Of The European Association For Computational Linguistics		Author:McDonald, Ryan
A possible direction of research is to investigate multilabel learning techniques for structured data (McDonald et al. , 2005a) that learn a scoring function separating a set of valid answers from all invalid answers. (self citation)

Title:E06-1038		Conference:Conference Of The European Association For Computational Linguistics		Author:McDonald, Ryan
This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text (Sarawagi and Cohen, 2004; McDonald et al. , 2005a). (self citation)

Title:W06-2934		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Riedel, Sebastian; Cakici, Ruken; Meza-Ruiz, Ivan
2.1 Decoding Instead of using the MST algorithm (McDonald et al. , 2005b) to maximise equation 1, we present an equivalent ILP formulation of the problem.

Title:W06-2934		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Riedel, Sebastian; Cakici, Ruken; Meza-Ruiz, Ivan
c2006 Association for Computational Linguistics Multi-lingual Dependency Parsing with Incremental Integer Linear Programming Sebastian Riedel and Ruket C akc and Ivan Meza-Ruiz ICCS School of Informatics University of Edinburgh Edinburgh, EH8 9LW, UK S.R.Riedel,R.Cakici,I.V.Meza-Ruiz@sms.ed.ac.uk Abstract Our approach to dependency parsing is based on the linear model of McDonald et al.(McDonald et al. , 2005b).

Title:W06-2934		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Riedel, Sebastian; Cakici, Ruken; Meza-Ruiz, Ivan
Our parser is inspired by McDonald et al.(2005a) which treats the task as the search for the highest scoring Maximum Spanning Tree (MST) in a graph.

Title:W06-2937		Conference:Conference On Computational Natural Language Learning CoNLL		Author:Wu, Yu-Chieh; Lee, Yue-Shi; Yang, Jie-Chi
2 System Description 241 Over the past decades, many state-of-the-art parsing algorithm were proposed, such as head-word lexicalized PCFG (Collins, 1998), Maximum Entropy (Charniak, 2000), Maximum/Minimum spanning tree (MST) (McDonald et al. , 2005), Bottom-up deterministic parsing (Yamada and Matsumoto, 2003), and Constant-time deterministic parsing (Nivre, 2003).

Title:N07-3002		Conference:NAACL-HLT 2007 Doctoral Consortium		Author:Wang, Qin Iris
Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al. , 2004; McDonald et al. , 2005), which demonstrates the state of the art performance in English dependency parsing.

Title:N07-3002		Conference:NAACL-HLT 2007 Doctoral Consortium		Author:Wang, Qin Iris
In which case, the parsing problem reduces to a18a37a36 a1a39a38a41a40a43a42a45a44a46a38a48a47 a49a22a50a41a51a53a52a55a54a57a56 a58 a52a60a59a62a61a5a63a64a59a66a65a43a56a67a50a26a49 sa2a5a4a29a19a22a21a23a4a25a24a28a16 (1) where the score sa2a5a4 a19 a21 a4 a24 a16 can depend on any measurable property of a4a20a19 and a4a25a24 within the tree a18 . This formulation is suf ciently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al. , 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al. , 2005; Wang et al. , 2006).

Title:N07-3002		Conference:NAACL-HLT 2007 Doctoral Consortium		Author:Wang, Qin Iris
1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al. , 2005; McDonald et al. , 2005).

Title:N07-3002		Conference:NAACL-HLT 2007 Doctoral Consortium		Author:Wang, Qin Iris
Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach the structured margin loss (McDonald et al. , 2005) is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component.

Title:N07-3002		Conference:NAACL-HLT 2007 Doctoral Consortium		Author:Wang, Qin Iris
Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al. , 2005), a suf ciently uni ed view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches.

Title:N07-3002		Conference:NAACL-HLT 2007 Doctoral Consortium		Author:Wang, Qin Iris
The structured large margin approach, on the other hand, uses a global scoring function by minimizing a training loss the structured margin loss (McDonald et al. , 2005) which is directly coordinated with the global tree.

Title:D07-1126		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nguyen, Minh Le; Shimazu, Akira; Nguyen, Thai Phuong; Phan, Xuan-Hieu
Table 5 reports experimental results by using the first order decoding method in which an MST parsing algorithm (McDonald et al. , 2005b) is applied for non-projective parsing and the Eisners method is used for projective language data.

Title:D07-1126		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nguyen, Minh Le; Shimazu, Akira; Nguyen, Thai Phuong; Phan, Xuan-Hieu
This feature was particularly helpful for nouns identifying their parent (McDonald et al. , 2005a).

Title:D07-1126		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nguyen, Minh Le; Shimazu, Akira; Nguyen, Thai Phuong; Phan, Xuan-Hieu
The performance of MIRA based parsing achieves the state-ofthe-art performance in English data (McDonald et al. , 2005a; McDonald et al. , 2006).

Title:D07-1126		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nguyen, Minh Le; Shimazu, Akira; Nguyen, Thai Phuong; Phan, Xuan-Hieu
1 Introduction Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al. , 2006) and discriminative learning methods (McDonald et al. , 2005a; Corston-Oliver et al. , 2006).

Title:D07-1126		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nguyen, Minh Le; Shimazu, Akira; Nguyen, Thai Phuong; Phan, Xuan-Hieu
For dependency parsing domain, McDonald et al (2005a) modified the MIRA learning algorithm (McDonald et al. , 2005a) for structured domains in which the optimization problem can be solved by using Hidreths algorithm (Censor and Zenios, 1997), which is faster than the quadratic programming technique.

Title:D07-1126		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nguyen, Minh Le; Shimazu, Akira; Nguyen, Thai Phuong; Phan, Xuan-Hieu
The algorithms are commonly used in other online-learning dependency parsing, such as in (McDonald et al. , 2005a).

Title:D07-1126		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nguyen, Minh Le; Shimazu, Akira; Nguyen, Thai Phuong; Phan, Xuan-Hieu
It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm (PC) (Collins, 2002) as well as the MIRA algorithm (McDonald et al. , 2005a) is in line 9.

Title:D07-1126		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nguyen, Minh Le; Shimazu, Akira; Nguyen, Thai Phuong; Phan, Xuan-Hieu
Averaging has been shown to help reduce overfitting (McDonald et al. , 2005a; Collins, 2002).

Title:D07-1100		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Nakagawa, Tetsuji
The best projective parse tree is obtained using the Eisner algorithm (Eisner, 1996) with the scores, and the best non-projective one is obtained using the ChuLiu-Edmonds (CLE) algorithm (McDonald et al. , 2005b).

Title:D07-1003		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Wang, Mengqiu; Smith, Noah A.; Mitamura, Teruko
The resulting POS-tagged sentences were then parsed using MSTParser (McDonald et al. , 2005), trained on the entire Penn Treebank to produce labeled dependency parse trees (we used a coarse dependency label set that includes twelve label types).

Title:D07-1003		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Wang, Mengqiu; Smith, Noah A.; Mitamura, Teruko
The tree is produced by a state-of-the-art dependency parser (McDonald et al. , 2005) trained on the Wall Street Journal Penn Treebank (Marcus et al. , 1993).

Title:P07-1050		Conference:45th Annual Meeting of the Association of Computational Linguistics		Author:Hall, Keith
1 Introduction The Maximum Spanning Tree algorithm1 was recently introduced as a viable solution for nonprojective dependency parsing (McDonald et al. , 2005b).

Title:D07-1013		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:McDonald, Ryan; Nivre, Joakim
Ax = {(i,j,l) | i,j Vx and l L} Let D(Gx) represent the subgraphs of graph Gx that are valid dependency graphs for the sentence x. Since Gx contains all possible labeled arcs, the set D(Gx) must necessarily contain all valid dependency graphs for x. Assume that there exists a dependency arc scoring function, s : V V L R. Furthermore, define the score of a graph as the sum of its arc scores, s(G = (V,A)) = summationdisplay (i,j,l)A s(i,j,l) The score of a dependency arc, s(i,j,l) represents the likelihood of creating a dependency from word wi to word wj with the label l. If the arc score function is known a priori, then the parsing problem can be stated as, 123 G = argmax GD(Gx) s(G) = argmax GD(Gx) summationdisplay (i,j,l)A s(i,j,l) This problem is equivalent to finding the highest scoring directed spanning tree in the graph Gx originatingoutoftherootnode0, whichcanbesolvedfor both the labeled and unlabeled case in O(n2) time (McDonald et al. , 2005b). (self citation)

Title:D07-1013		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:McDonald, Ryan; Nivre, Joakim
To learn arc scores, these models use large-margin structured learning algorithms (McDonald et al. , 2005a), which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set. (self citation)

Title:W07-1509		Conference:Linguistic Annotation Workshop		Author:Ganchev, Kuzman; Pereira, Fernando; Mandel, Mark; Carroll, Steven; White, Peter
Our experiments use the MIRA algorithm (Crammer et al. , 2006; McDonald et al. , 2005) to learn the weight vector. (self citation)

Title:W07-1509		Conference:Linguistic Annotation Workshop		Author:Ganchev, Kuzman; Pereira, Fernando; Mandel, Mark; Carroll, Steven; White, Peter
We used a k53 best version of the MIRA algorithm (Crammer et al. , 2006; McDonald et al. , 2005). (self citation)

Title:P07-2052		Conference:45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions		Author:Kawahara, Daisuke; Uchimoto, Kiyotaka
where S is the stack that keeps the words being under consideration, I is the list of reDA RA CR (Yamada and Matsumoto, 2003) 90.3 91.6 38.4 (Nivre and Scholz, 2004) 87.3 84.3 30.4 (Isozaki et al. , 2004) 91.2 95.7 40.7 (McDonald et al. , 2005) 90.9 94.2 37.5 (McDonald and Pereira, 2006) 91.5 N/A 42.1 (Corston-Oliver et al. , 2006) 90.8 93.7 37.6 Our Base Parser 90.9 92.6 39.2 Table 2: Comparison of parser performance.

Title:P07-2052		Conference:45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions		Author:Kawahara, Daisuke; Uchimoto, Kiyotaka
McDonald et al. proposed an online large-margin method for training dependency parsers (McDonald et al. , 2005).

Title:P07-2052		Conference:45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions		Author:Kawahara, Daisuke; Uchimoto, Kiyotaka
Dependency parsing has been actively studied in recent years (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Isozaki et al. , 2004; McDonald et al. , 2005; McDonald and Pereira, 2006; Corston-Oliver et al. , 2006).

Title:D07-1070		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Smith, David A.; Eisner, Jason M.
We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features, as in recent models for scoring dependency parses (McDonald et al. , 2005).

Title:P07-1055		Conference:45th Annual Meeting of the Association of Computational Linguistics		Author:McDonald, Ryan; Hannan, Kerry; Neylon, Tyler; Wells, Mike; Reynar, Jeffrey C.
These algorithms are usually applied to sequential labeling or chunking, but have also been applied to parsing (Taskar et al. , 2004; McDonald et al. , 2005), machine translation (Liang et al. , 2006) and summarization (Daume III et al. , 2006). (self citation)

Title:P07-1055		Conference:45th Annual Meeting of the Association of Computational Linguistics		Author:McDonald, Ryan; Hannan, Kerry; Neylon, Tyler; Wells, Mike; Reynar, Jeffrey C.
Hidden Markov models (Rabiner, 1989) are one of the earliest structured learning algorithms, which have recently been followedbydiscriminativelearningapproachessuch as conditional random fields (CRFs) (Lafferty et al. , 2001; Sutton and McCallum, 2006), the structured perceptron (Collins, 2002) and its large-margin variants (Taskar et al. , 2003; Tsochantaridis et al. , 2004; McDonald et al. , 2005; Daume III et al. , 2006). (self citation)

Title:P07-1055		Conference:45th Annual Meeting of the Association of Computational Linguistics		Author:McDonald, Ryan; Hannan, Kerry; Neylon, Tyler; Wells, Mike; Reynar, Jeffrey C.
2.1.3 Training the Model Let Y = Y(d) Y(s)n be the set of all valid sentence-document labelings for an input s. The weights, w, are set using the MIRA learning algorithm, which is an inference based online largemargin learning technique (Crammer and Singer, 2003; McDonald et al. , 2005). (self citation)

Title:P07-1055		Conference:45th Annual Meeting of the Association of Computational Linguistics		Author:McDonald, Ryan; Hannan, Kerry; Neylon, Tyler; Wells, Mike; Reynar, Jeffrey C.
The constraint set C can be chosen arbitrarily, but it is usually taken to be the k labelings that have the highest score under the old weight vector w(i) (McDonald et al. , 2005). (self citation)

Title:W07-2216		Conference:Tenth International Conference on Parsing Technologies		Author:McDonald, Ryan; Satta, Giorgio
In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al. , 2005a; McDonald et al. , 2005b). (self citation)

Title:W07-2216		Conference:Tenth International Conference on Parsing Technologies		Author:McDonald, Ryan; Satta, Giorgio
Let R(T,Tprime) be the Hamming distance between two dependency graphs for an input sentence x = x0x1 xn, R(T,Tprime) = n summationdisplay (i,j)kET I((i,j)k,Tprime) This is a common definition of risk between two graphs as it corresponds directly to labeled dependency parsing accuracy (McDonald et al. , 2005a; Buchholz et al. , 2006). (self citation)

Title:W07-2216		Conference:Tenth International Conference on Parsing Technologies		Author:McDonald, Ryan; Satta, Giorgio
Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al. , 2005a). (self citation)

Title:W07-2216		Conference:Tenth International Conference on Parsing Technologies		Author:McDonald, Ryan; Satta, Giorgio
These include the perceptron (Collins, 2002) and its large-margin variants (Crammer and Singer, 2003; McDonald et al. , 2005a). (self citation)

Title:W07-2216		Conference:Tenth International Conference on Parsing Technologies		Author:McDonald, Ryan; Satta, Giorgio
1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al. , 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and Novak, 2005; McDonald et al. , 2005b). (self citation)

Title:W07-2216		Conference:Tenth International Conference on Parsing Technologies		Author:McDonald, Ryan; Satta, Giorgio
Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al. , 2005b). (self citation)

Title:D07-1080		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Watanabe, Taro; Suzuki, Jun; Tsukada, Hajime; Isozaki, Hideki
MIRA is successfully employed in dependency parsing (McDonald et al. , 2005) or the joint-labeling/chunking task (Shimizu and Haas, 2006).

Title:D07-1080		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Watanabe, Taro; Suzuki, Jun; Tsukada, Hajime; Isozaki, Hideki
4.1 Margin Infused Relaxed Algorithm The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al. , 2006) is an online version of the large-margin training algorithm for structured classification (Taskar et al. , 2004) that has been successfully used for dependency parsing (McDonald et al. , 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).

Title:D07-1101		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Carreras, Xavier
In turn, those features were inspired by successful previous work in firstorder dependency parsing (McDonald et al. , 2005).

Title:D07-1033		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Kazama, Jun'ichi; Torisawa, Kentaro
Such methods include ALMA (Gentile, 2001) used in (Daume III and Marcu, 2005)2, MIRA (Crammer et al. , 2006) used in (McDonald et al. , 2005), and Max-Margin Markov Networks (Taskar et al. , 2003).

Title:D07-1014		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Smith, David A.; Smith, Noah A.
Dependency parsing can be used to provide a bare bones syntactic structurethatapproximatessemantics,andithastheadditionaladvantageofadmittingfastparsingalgorithms (Eisner, 1996; McDonald et al. , 2005b) with a negligible grammar constant in many cases.

Title:D07-1014		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Smith, David A.; Smith, Noah A.
133 than the margin (McDonald et al. , 2005a).

Title:D07-1014		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Smith, David A.; Smith, Noah A.
The latest state-of-the-art statistical dependency parsers are discriminative, meaning that they are based on classifiers trained to score trees, given a sentence, either via factored whole-structure scores (McDonald et al. , 2005a) or local parsing decision scores (Hall et al. , 2006).

Title:D07-1015		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Koo, Terry; Globerson, Amir; Carreras, Xavier; Collins, Michael John
These structures are equivalent to non-projective dependency parses (McDonald et al. , 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree.

Title:D07-1015		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Koo, Terry; Globerson, Amir; Carreras, Xavier; Collins, Michael John
This contrasts with the online learning algorithms used in previous work with spanning-tree models (McDonald et al. , 2005b).

Title:D07-1127		Conference:2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)		Author:Schiehlen, Michael; Spranger, Kristina
One promising approach is based on exact search and structural learning (McDonald et al. , 2005; McDonald and Pereira, 2006).

Title:C08-1046		Conference:International Conference On Computational Linguistics		Author:Iwatate, Masakazu; Asahara, Masayuki; Matsumoto, Yuji
4 Experiment 4.1 Settings We implemented the tournament model, the CC algorithm (Kudo and Matsumoto, 2002), SR algorithm (Sassano, 2004) and CLE algorithm (McDonald et al., 2005) with SVM classifiers.

Title:C08-1046		Conference:International Conference On Computational Linguistics		Author:Iwatate, Masakazu; Asahara, Masayuki; Matsumoto, Yuji
365 Method Features Jan. 9th Jan.10th Jan. 15th Tournament Standard feature only 89.89/49.63 89.63/48.34 89.40/49.70 All features 90.09/49.71 90.11/49.02 90.35/52.59 SR algorithm Standard feature only 88.18/45.92 88.80/44.76 88.03/47.24 (Sassano, 2004) All features 89.22/47.90 89.79/47.87 89.55/49.79 CC algorithm Standard feature only 88.17/45.92 88.80/44.76 88.00/47.24 (Kudo and Matsumoto, 2002) All features 89.22/47.90 89.80/47.94 89.53/49.79 CLE algorithm Standard feature only 88.64/45.34 88.16/43.14 88.07/45.21 (McDonald et al., 2005) Standard and Additional 89.21/46.83 89.05/45.03 88.90/48.43 Table 2: Dependency and sentence accuracy [%] using 7,587 sentences as training data.

Title:P08-1108		Conference:Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue		Author:Nivre, Joakim; McDonald, Ryan
More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f(i,j,l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al., 2005a; McDonald et al., 2006). (self citation)

Title:P08-1108		Conference:Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue		Author:Nivre, Joakim; McDonald, Ryan
As a result, the dependency parsing problem is written: G = argmax G=(V,A) summationdisplay (i,j,l)A s(i,j,l) This problem is equivalent to finding the highest scoring directed spanning tree in the complete graph over the input sentence, which can be solved in O(n2) time (McDonald et al., 2005b). (self citation)

Title:P08-1108		Conference:Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue		Author:Nivre, Joakim; McDonald, Ryan
An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a). (self citation)

Title:D08-1052		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Shen, Libin; Joshi, Aravind K.
Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005).

Title:D08-1052		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Shen, Libin; Joshi, Aravind K.
23 of the LTAG Treebank dency is comparable to the numbers of the previous best systems on dependency extracted from PTB with Magermans rules, for example, 90.3% in (Yamada and Matsumoto, 2003) and 90.9% in (McDonald et al., 2005).

Title:C08-1050		Conference:International Conference On Computational Linguistics		Author:Johansson, Richard; Nugues, Pierre
A dependency-based system using MSTParser (McDonald et al., 2005).

Title:C08-1050		Conference:International Conference On Computational Linguistics		Author:Johansson, Richard; Nugues, Pierre
The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005).

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
1, on which we focus in this paper: Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as arc factorization for nonprojective dependency parsing (McDonald and Satta, 2007).

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
Such models are commonlyreferredto as edge-factored since their parametersfactor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a).Edge-factoredmodelshave many computationalbenefits,mostnotablythat inferencefor nonprojective dependency graphs can be achieved in polynomialtime(McDonaldet al., 2005b).Theprimary problem in treating each dependency as independentis that it is not a realistic assumption.

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
1.1 RelatedWork There has been extensive work on data-driven dependency parsingfor both projective parsing(Eisner, 1996; Paskin, 2001; Yamadaand Matsumoto, 2003; Nivre and Scholz, 2004; McDonaldet al., 2005a) and non-projective parsingsystems(Nivre and Nilsson,2005;Halland Novak, 2005;McDonald et al., 2005b).

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
In graph-based parsing, dependency trees are scored by factoring the tree into its arcs, and parsing is performed by searching for the highest scoring tree (Eisner, 1996; McDonald et al., 2005b).

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b).

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
Such models are commonlyreferredto as edge-factored since their parametersfactor relative to individualedges of the graph (Paskin, 2001; McDonald et al., 2005a).Edge-factoredmodelshave many computationalbenefits,mostnotablythatinferencefor nonprojective dependency graphs can be achieved in polynomialtime(McDonaldet al.,2005b).Theprimary problemin treating each dependency as independentis that it is not a realisticassumption.

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
In this work we use two well-known, publicly available dependency parsers, MSTParser (McDonald et al., 2005b),1 which implements ex1http://sourceforge.net/projects/mstparser 159 act first-order arc-factored nonprojective parsing (2.1.2) and approximate second-order nonprojective parsing, and MaltParser (Nivre et al., 2006), which is a state-of-the-art transition-based parser.2 We do not alter the training algorithms used in prior work for learning these two parsers from data.

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007).

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
1.1 RelatedWork There has been extensive work on data-driven dependency parsingfor both projective parsing(Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonaldet al., 2005a) and non-projective parsing systems (Nivre and Nilsson,2005;Hall and Novak, 2005;McDonald et al., 2005b).

Title:D08-1017		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Torres Martins, Andr&eacute;; Filipe; Das, Dipanjan; Smith, Noah A.; Xing, Eric P.
We adapted this system to first perform unlabeled parsing, then label the arcs using a log-linear classifier with access to the full unlabeled parse (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006).

Title:D08-1059		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Zhang, Yue; Clark, Stephen
2 The graph-based parser Following MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006), we define the graphVariables: agenda the beam for state items item partial parse tree output a set of output items index,prev word indexes Input: x POS-tagged input sentence.

Title:D08-1059		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Zhang, Yue; Clark, Stephen
1 Introduction Graph-based (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006) and transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms offer two different approaches to data-driven dependency parsing.

Title:D08-1059		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Zhang, Yue; Clark, Stephen
We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of vectorw.

Title:D08-1059		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Zhang, Yue; Clark, Stephen
Rows MSTParser 1/2 show the first-order (using feature templates 1 5 from Table 1) (McDonald et al., 2005) and secondorder (using all feature templates from Table 1) (McDonald and Pereira, 2006) MSTParsers, as reported by the corresponding papers.

Title:J08-4003		Conference:Computational Linguistics		Author:Nivre, Joakim
This approach has been further developed in particular by Ryan McDonald and his colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006) and is now known as spanning tree parsing, because the problem of nding the most probable tree under this type of model is equivalent to nding an optimum spanning tree in a dense graph containing all possible dependency arcs.

Title:W08-2131		Conference:CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning		Author:Yuret, Deniz; Yatbaz, Mehmet Ali; Ural, Ahmet Engin
The parameters were determined based on the experimental results of the English task in (McDonald et al., 2005), i.e. we used projective parsing and a first order feature set during training.

Title:P08-1067		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Huang, Liang
Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005).

Title:D08-1016		Conference:Conference On Empirical Methods In Natural Language Processing		Author:Smith, David A.; Eisner, Jason M.
A first-order (or edge-factored) parsing model (McDonald et al., 2005) contains only LINK factors, along with a global TREE or PTREE factor.

Title:C08-5001		Conference:COLING - Tutorials		Author:Huang, Liang
Applications of this algorithm include k-best parsing (McDonald et al., 2005; Mohri and Roark, 2006) and machine translation (Chiang, 2007).

Title:C08-5001		Conference:COLING - Tutorials		Author:Huang, Liang
The k-best list is also frequently used in discriminative learning to approximate the whole set of candidates which is usually exponentially large (Och, 2003; McDonald et al., 2005).

Title:P08-1061		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Wang, Qin Iris; Schuurmans, Dale; Lin, Dekang
1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007).

Title:P08-1061		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Wang, Qin Iris; Schuurmans, Dale; Lin, Dekang
3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al., 2004; McDonald et al., 2005a; Wang et al., 2006).

Title:P08-1061		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Wang, Qin Iris; Schuurmans, Dale; Lin, Dekang
Using the full set of features described in (McDonald et al., 2005a; Wang et al., 2007) and comparing the corresponding dependency parsing 537 English PTB-10 Training(l/ul) 3026/1016 Dev 163 Test 270 PTB-15 Training 7303/2370 Dev 421 Test 603 PTB-20 Training 12519/4003 Dev 725 Test 1034 Chinese CTB4-10 Training(l/ul) 642/347 Dev 61 Test 40 CTB4-15 Training 1262/727 Dev 112 Test 83 CTB4-20 Training 2038/1150 Dev 163 Test 118 CTB4-40 Training 4400/2452 Dev 274 Test 240 CTB4 Training 5314/2977 Dev 300 Test 289 Table 1: Size of Experimental Data (# of sentences) results with previous work remains a direction for future work.

Title:P08-1061		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Wang, Qin Iris; Schuurmans, Dale; Lin, Dekang
Features For simplicity, in current work, we only used two sets of featuresword-pair and tag-pair indicator features, which are a subset of features used by other researchers on dependency parsing (McDonald et al., 2005a; Wang et al., 2007).

Title:P08-1061		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Wang, Qin Iris; Schuurmans, Dale; Lin, Dekang
Dependency Parsing Algorithms For simplicity of implementation, we use a standard CKY parser in the experiments, although Eisners algorithm (Eisner, 1996) and the Spanning Tree algorithm (McDonald et al., 2005b) are also applicable.

Title:P08-1061		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Wang, Qin Iris; Schuurmans, Dale; Lin, Dekang
In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a).

Title:P08-1061		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Wang, Qin Iris; Schuurmans, Dale; Lin, Dekang
Given this assumption, the parsing problem reduces to find Y = arg max Y(X) score(Y|X) (1) = arg max Y(X) summationdisplay (xixj)Y score(xi xj) where the score(xi xj) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a).

Title:P08-1061		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Wang, Qin Iris; Schuurmans, Dale; Lin, Dekang
This approach corresponds to the training problem posed in (McDonald et al., 2005a) and has yielded the best published results for English dependency parsing.

Title:I08-2119		Conference:Proceedings of the Third International Joint Conference on Natural Language Processing		Author:Wang, Mengqiu
Corpus preprocessing is done as the following: sentence segmentation was performed using the tool from CCG group at UIUC 1; words are then tokenized and tagged with part-of-speech using MXPOST (Ratnaparkhi, 1996) and dependency parsing is performed using MSTParser (McDonald et al., 2005a).

Title:I08-2119		Conference:Proceedings of the Third International Joint Conference on Natural Language Processing		Author:Wang, Mengqiu
The second type of collection that has been widely studied is biomedical literature (Bunescu and Mooney, 2005b; Giuliano et al., 2006; McDonald et al., 2005b), promoted by evaluation programs such as BioCreAtIvE and JNLPBA 2004.

Title:W08-2102		Conference:CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning		Author:Carreras, Xavier; Collins, Michael John; Koo, Terry
The dependencies D must form a directed, projective tree spanning words 0n, with at the root of this tree, as is also the case in previous work on discriminative approches to dependency parsing (McDonald et al., 2005).

Title:W08-2102		Conference:CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning		Author:Carreras, Xavier; Collins, Michael John; Koo, Terry
The first-stage model we use is a first-order dependency model, with labeled dependencies, as described in (McDonald et al., 2005).

Title:P08-1066		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Shen, Libin; Xu, Jinxi; Weischedel, Ralph M.
6 Discussion The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005).

Title:P08-1110		Conference:Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue		Author:G&oacute;mez-Rodr&iacute;guez, Carlos; Carroll, John; Weir, David
This is the principle behind the parser defined by Eisner (1996), which is still in wide use today (Corston-Oliver et al., 2006; McDonald et al., 2005a).

Title:P08-1068		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Koo, Terry; Carreras, Xavier; Collins, Michael John
(1999), we used a coarsened version of the Czech part of speech tags; this choice also matches the conditions of previous work (McDonald et al., 2005b; McDonald and Pereira, 2006).

Title:P08-1068		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Koo, Terry; Carreras, Xavier; Collins, Michael John
The data partition and head rules were chosen to match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006).

Title:P08-1068		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Koo, Terry; Carreras, Xavier; Collins, Michael John
Czech dependency structures may contain nonprojective edges, so we employ a maximum directed spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b) as our firstorder parser for Czech.

Title:P08-1068		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Koo, Terry; Carreras, Xavier; Collins, Michael John
For many different part factorizations and structure domains Y(), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased contextsensitivity (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007).

Title:P08-1068		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Koo, Terry; Carreras, Xavier; Collins, Michael John
The feature sets we used are similar to other feature sets in the literature (McDonald et al., 2005a; Carreras, 2007), so we will not attempt to give a exhaustive description of the features in this section.

Title:P08-1068		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Koo, Terry; Carreras, Xavier; Collins, Michael John
In addition, in English parsing we ignore the parent-predictions of punctuation tokens,13 and in Czech parsing we retain the punctuation tokens; this matches previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006).

Title:P08-1068		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Koo, Terry; Carreras, Xavier; Collins, Michael John
To facilitate comparisons with previous work (McDonald et al., 2005b; McDonald and Pereira, 2006), we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus.10 Czech word clusters were derived from the raw text section of the PDT 1.0, which contains about 39 million words of newswire text.11 We trained the parsers using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which represents a balance between strong performance and fast training times.

Title:P08-1007		Conference:Annual Meeting Of The Association For Computational Linguistics		Author:Chan, Yee Seng; Ng, Hwee Tou
In our work, we train the MSTParser4 (McDonald et al., 2005) on the Penn Treebank Wall Street Journal (WSJ) corpus, and use it to extract dependency relations from a sentence.

Title:W09-1215		Conference:Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task		Author:Ren, Han; Donghong, Ji; Wan, Jing; Zhang, Mingyao
Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the goal is to find the dependency tree with highest score.

Title:N09-3011		Conference:HLT-NAACL, Companion Volume: Student Research Workshop and Doctoral Consortium		Author:Bellare, Kedar; Crammer, Koby; Freitag, Dayne
The training algorithm we propose in this paper is based on the K-best MIRA algorithm which has been used earlier in structured prediction problems (McDonald et al., 2005a; McDonald et al., 2005b). (self citation)

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
This modification is essential inordertomakeourparserrunintrueO(n2)time,asopposed to (McDonald et al., 2005b).

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
6 Conclusion and future work In this paper, we presented a non-projective dependency parser whose time-complexity of O(n2) improves upon the cubic time implementation of (McDonald et al., 2005b), and does so with little loss in dependency accuracy (.25% to .34%).

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
2.1 O(n2)-time dependency parsing for MT We now formalize weighted non-projective dependency parsing similarly to (McDonald et al., 2005b) and then describe a modified and more efficient version that can be integrated into a phrasebased decoder.

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
Since i and j are both free variables, feature computation in (McDonald et al., 2005b) takes time O(n3), even though parsing itself takes O(n2) time.

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
We score each dependency relation using a standard linear model s(i, j) = f(i, j) (1) whose weight vector is trained using MIRA (Crammer and Singer, 2003) to optimize dependency parsing accuracy (McDonald et al., 2005a).

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
The O(n3) non-projective parser of (McDonald et al., 2005b) is slightly more accurate than our version, though ours runs in O(n2) time.

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
2 Dependency parsing for machine translation In this section, we review dependency parsing formulated as a maximum spanning tree problem (McDonald et al., 2005b), which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding.

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
In the caseofdependencyparsingforCzech,(McDonald et al., 2005b) even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing.

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
Thatisnotthecase: dependencyaccuracyfornonprojective parsing is 90.2% for English (McDonald et al., 2005b), only 0.7% lower than a projective parser (McDonald et al., 2005a) that uses the same set of features and learning algorithm.

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
The training data consists of about 28 million English words and 23.3 million 5Note that our results on WSJ are not exactly the same as those reported in (McDonald et al., 2005b), since we used slightly different head finding rules.

Title:P09-1087		Conference:ACL-IJCNLP		Author:Galley, Michel; Manning, Christopher D.
Table 4 shows that the accuracy of our truly 777 O(n2) parser is only .25% to .34% worse than the O(n3) implementation of (McDonald et al., 2005b).5 Compared to the state-of-the-art projective parser as implemented in (McDonald et al., 2005a), performance is 1.28% lower on WSJ, but only 0.95% when training on all our available data and using the MT setting.

Title:N09-1069		Conference:HLT-NAACL		Author:Liang, Percy; Klein, Dan
One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few.

Title:P09-2026		Conference:ACL-IJCNLP: Short Papers		Author:Chaudhuri, Sourish; Gupta, Naman K.; Smith, Noah A.; Ros&eacute;, Carolyn Penstein
The system uses as input the paired corpus, the corresponding POS tagged corpus, the paired corpus parsed using the Charniak parser (Charniak, 2000), and dependency parses from the MST parser (McDonald et al., 2005).

Title:D09-1085		Conference:EMNLP		Author:Rimell, Laura; Clark, Stephen; Steedman, Mark
Also, dependency parsers are not significantly better at recovering head-based dependencies than constituent parsers based on the PTB (McDonald et al., 2005).

Title:D09-1085		Conference:EMNLP		Author:Rimell, Laura; Clark, Stephen; Steedman, Mark
The RASP parser is based on a manually constructed POS tag-sequence grammar, with a statistical parse selection component and a robust 1One obvious omission is any form of dependency parser (McDonald et al., 2005; Nivre and Scholz, 2004).

Title:P09-1093		Conference:ACL-IJCNLP		Author:Hirao, Tsutomu; Suzuki, Jun; Isozaki, Hideki
For example, w is optimized as follows: w(new) = w(old) epsilon1 L w(old) (11) 829 Our parameter optimization procedure can be replaced by another one such as MIRA (McDonald et al., 2005) or CRFs (Lafferty et al., 2001).

Title:P09-1040		Conference:ACL-IJCNLP		Author:Nivre, Joakim
Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b).

Title:P09-1040		Conference:ACL-IJCNLP		Author:Nivre, Joakim
Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover non351 ROOT0 A1 a7 a4 a63 DET hearing2 a7 a4 a63 SBJ is3 a7 a4 a63 ROOT scheduled4 a7 a4 a63 VG on5 a7 a4 a63 NMOD the6 a7 a4 a63 DET issue7 a7 a4 a63 PC today8 a7 a4 a63 ADV .9 a63 a7 a4P Figure 1: Dependency tree for an English sentence (non-projective).

Title:P09-1040		Conference:ACL-IJCNLP		Author:Nivre, Joakim
1 Introduction Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007).

Title:D09-1021		Conference:EMNLP		Author:Carreras, Xavier; Collins, Michael John
In particular, we will consider discriminative models (analogous to models for dependency parsing, e.g., see (McDonald et al., 2005)) that estimate the probability of targetlanguage dependencies conditioned on properties of the source-language string.

Title:D09-1021		Conference:EMNLP		Author:Carreras, Xavier; Collins, Michael John
Inspired by work in discriminative dependency parsing (e.g., (McDonald et al., 2005)), we add probabilistic constraints to the model through a discriminative model that links lexical dependencies in the target language to features of the source language string.

Title:P09-1059		Conference:ACL-IJCNLP		Author:Jiang, Wenbin; Huang, Liang; Liu, Qun
This is similar to feature design in discriminative dependency parsing (McDonald et al., 2005; Mc525 Donald and Pereira, 2006), where the basic features, composed of words and POSs in the context, are also conjoined with link direction and distance in order to obtain more special features.

Title:P09-1041		Conference:ACL-IJCNLP		Author:Druck, Gregory; Mann, Gideon S.; McCallum, Andrew
The second full set includes standard features for edgefactored dependency parsers (McDonald et al., 2005), though still unlexicalized.

Title:P09-1041		Conference:ACL-IJCNLP		Author:Druck, Gregory; Mann, Gideon S.; McCallum, Andrew
We also use standard edge-factored feature templates (McDonald et al., 2005)10.

Title:P09-1041		Conference:ACL-IJCNLP		Author:Druck, Gregory; Mann, Gideon S.; McCallum, Andrew
In some cases, the possible parent constraints described above will not be enough to provide high accuracy, because they do not consider other tags in the sentence (McDonald et al., 2005).

Title:P09-1043		Conference:ACL-IJCNLP		Author:Zhang, Yi; Wang, Rui
In combination with machine learning methods, several statistical dependency parsing models have reached comparable high parsing accuracy (McDonald et al., 2005b; Nivre et al., 2007b).

Title:P09-1043		Conference:ACL-IJCNLP		Author:Zhang, Yi; Wang, Rui
2 Parser Domain Adaptation In recent years, two statistical dependency parsing systems, MaltParser (Nivre et al., 2007b) and MSTParser (McDonald et al., 2005b), representing different threads of research in data-driven machine learning approaches have obtained high publicity, for their state-of-the-art performances in open competitions such as CoNLL Shared Tasks.

Title:P09-1043		Conference:ACL-IJCNLP		Author:Zhang, Yi; Wang, Rui
MSTParser, on the other hand, follows 378 the graph-based approach where the best parse tree is acquired by searching for a spanning tree which maximizes the score on either a partially or a fully connected graph with all words in the sentence as nodes (Eisner, 1996; McDonald et al., 2005b).

Title:D09-1058		Conference:EMNLP		Author:Suzuki, Jun; Isozaki, Hideki; Carreras, Xavier; Collins, Michael John
as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008).

Title:D09-1058		Conference:EMNLP		Author:Suzuki, Jun; Isozaki, Hideki; Carreras, Xavier; Collins, Michael John
In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: p(y|x) = 1Z(x) exp{g(x,y)}, (1) where Z(x) is the partition function, w is a parameter vector, and g(x,y) = summationdisplay (h,m,l)y wf(x,h,m,l) Here f(x,h,m,l) is a feature vector representing the dependency (h,m,l) in the context of the sentence x (see for example (McDonald et al., 2005a)).

Title:D09-1058		Conference:EMNLP		Author:Suzuki, Jun; Isozaki, Hideki; Carreras, Xavier; Collins, Michael John
These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008).

Title:D09-1058		Conference:EMNLP		Author:Suzuki, Jun; Isozaki, Hideki; Carreras, Xavier; Collins, Michael John
Our baseline features (baseline) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on.

Title:D09-1058		Conference:EMNLP		Author:Suzuki, Jun; Isozaki, Hideki; Carreras, Xavier; Collins, Michael John
Then, we substitute Step 3 as a supervised learning such as MIRA with a second-order parsing model (McDonald et al., 2005a), which incorporates q1 as a real-values features.

Title:D09-1058		Conference:EMNLP		Author:Suzuki, Jun; Isozaki, Hideki; Carreras, Xavier; Collins, Michael John
(a) English dependency parsers on PTB dependency parser test description (McDonald et al., 2005a) 90.9 1od (McDonald and Pereira, 2006) 91.5 2od (Koo et al., 2008) 92.23 1od, 43M ULD SS-SCM (w/ CL) 92.70 1od, 3.72G ULD (Koo et al., 2008) 93.16 2od, 43M ULD 2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD (b) Czech dependency parsers on PDT dependency parser test description (McDonald et al., 2005b) 84.4 1od (McDonald and Pereira, 2006) 85.2 2od (Koo et al., 2008) 86.07 1od, 39M ULD (Koo et al., 2008) 87.13 2od, 39M ULD SS-SCM (w/ CL) 87.14 1od, 39M ULD 2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD Table 6: Comparisons with the previous top systems: (1od, 2od: 1stand 2nd-order parsing model, ULD: unlabeled data).

Title:P09-1042		Conference:ACL-IJCNLP		Author:Ganchev, Kuzman; Gillenwater, Jennifer; Taskar, Ben
To get a feel for the typical case, we used off-the-shelf parsers (McDonald et al., 2005) for English, Spanish and Bulgarian on two bitexts (Koehn, 2005; Tiedemann, 2007) and compared several measures of dependency conservation.

Title:P09-1042		Conference:ACL-IJCNLP		Author:Ganchev, Kuzman; Gillenwater, Jennifer; Taskar, Ben
A parallel corpus is word-level aligned using an alignment toolkit (Graa et al., 2009) and the source (English) is parsed using a dependency parser (McDonald et al., 2005).

Title:P09-1042		Conference:ACL-IJCNLP		Author:Ganchev, Kuzman; Gillenwater, Jennifer; Taskar, Ben
Recently, dependency parsing has gained popularity as a simpler, computationally more efficient alternative to constituency parsing and has spurred several supervised learning approaches (Eisner, 1996; Yamada and Matsumoto, 2003a; Nivre and Nilsson, 2005; McDonald et al., 2005) as well as unsupervised induction (Klein and Manning, 2004; Smith and Eisner, 2006).

Title:P09-1042		Conference:ACL-IJCNLP		Author:Ganchev, Kuzman; Gillenwater, Jennifer; Taskar, Ben
The discriminative parser is based on the edge-factored model and features of the MSTParser (McDonald et al., 2005).

Title:P09-1007		Conference:ACL-IJCNLP		Author:Zhao, Hai; Song, Yan; Kitt, Chunyu; Zhou, Guodong
Although (McDonald et al., 2005) used the prefix of each word form instead of word form itself as features, character-level features here for Chinese is essentially different from that.

Title:P09-1007		Conference:ACL-IJCNLP		Author:Zhao, Hai; Song, Yan; Kitt, Chunyu; Zhou, Guodong
1 Introduction Although supervised learning methods bring stateof-the-art outcome for dependency parser inferring (McDonald et al., 2005; Hall et al., 2007), a large enough data set is often required for specific parsing accuracy according to this type of methods.

Title:D09-1086		Conference:EMNLP		Author:Smith, David A.; Eisner, Jason M.
We trained an edge-factored dependency parser (McDonald et al., 2005) on source domain data that followed one set of dependency conventions.

Title:D09-1086		Conference:EMNLP		Author:Smith, David A.; Eisner, Jason M.
In the conditional models of 3 and 6, these features are those of an edge-factored dependency parser (McDonald et al., 2005).

Title:P09-1053		Conference:ACL-IJCNLP		Author:Das, Dipanjan; Smith, Noah A.
8http://svmlight.joachims.org 9Our replication of the Wan et al. model is approximate, because we used different preprocessing tools: MXPOST for POS tagging (Ratnaparkhi, 1996), MSTParser for parsing (McDonald et al., 2005), and Dan Bikels interface (http://www.cis.upenn.edu/dbikel/ software.html#wn) to WordNet (Miller, 1995) for lemmatization information.

Title:N09-1068		Conference:HLT-NAACL		Author:Finkel, Jenny Rose; Manning, Christopher D.
We used the same features as (McDonald et al., 2005), augmented with information about whether or not a dependent is the first dependent (information they did not have).

Title:D09-1060		Conference:EMNLP		Author:Chen, Wenliang; Kazama, Jun'ichi; Uchimoto, Kiyotaka; Torisawa, Kentaro
To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23).

Title:W09-1412		Conference:Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task		Author:Georgiev, Georgi; Ganchev, Kuzman; Momchev, Vassil; Peychev, Deyan; Nakov, Preslav; Roberts, Angus
We further used some simple features from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al., 2005), extracted using parsers trained on Genia corpora.

Title:W09-1412		Conference:Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task		Author:Georgiev, Georgi; Ganchev, Kuzman; Momchev, Vassil; Peychev, Deyan; Nakov, Preslav; Roberts, Angus
We used a one-best version of MIRA (Crammer, 2004; McDonald et al., 2005) to choose w. MIRA is an online learning algorithm that updates the weight vector w for each training sentence xi according to the following rule: 95 wnew = argmin w bardblw woldbardbl s.t. w f(xi,yi) w f(x, y) L(yi, y) where L(yi,y) is a measure of the loss of using y instead of the correct labeling yi, and y is a shorthand for ywold(xi).

Title:W09-1212		Conference:Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task		Author:Llu&iacute;s, Xavier; Bott, Stefan; M&agrave;rquez, Llu&iacute;s
These features are borrowed from existing and widely-known systems (Xue and Palmer, 2004; McDonald et al., 2005; Carreras et al., 2006; Surdeanu et al., 2007).

Title:D09-1127		Conference:EMNLP		Author:Huang, Liang; Jiang, Wenbin; Liu, Qun
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those stateof-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16).

Title:P10-1037		Conference:ACL		Author:Sassano, Manabu; Kurohashi, Sadao
(2008) compare their proposed algorithm with various ones that include Sassanos, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005).

Title:W10-3005		Conference:Proceedings of the Fourteenth Conference on Computational Natural Language Learning		Author:Ji, Feng; Qiu, Xipeng; Huang, Xuanjing
Moreover, we train a first-order projective dependency parser with MSTParser5 (McDonald et al., 2005) on the standard WSJ training corpus, which is converted from constituent trees to dependency trees by several heuristic rules6.

Title:W10-3005		Conference:Proceedings of the Fourteenth Conference on Computational Natural Language Learning		Author:Ji, Feng; Qiu, Xipeng; Huang, Xuanjing
These features are generated by a first-order projective dependency parser (McDonald et al., 2005), and listed in Figure 3.

Title:W10-1412		Conference:Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages		Author:Goldberg, Yoav; Elhadad, Michael
It was shown to be more accurate than MALTPARSER, a state-of-the-art transition based parser (Nivre et al., 2006), and near the performance of the first-order MSTPARSER, a graph based parser which decomposes its score over tree edges (McDonald et al., 2005), while being more efficient.

Title:N10-1138		Conference:Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics		Author:Das, Dipanjan; Schneider, Nathan; Chen, Desai; Smith, Noah A.
We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents.

Title:P10-2035		Conference:Proceedings of the ACL 2010 Conference Short Papers		Author:Kitagawa, Kotaro; Tanaka-Ishii, Kumiko
Global-optimization parsing methods are another common approach (Eisner, 1996; McDonald et al., 2005).

Title:P10-1110		Conference:ACL		Author:Huang, Liang; Sagae, Kenji
The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers.

Title:P10-1110		Conference:ACL		Author:Huang, Liang; Sagae, Kenji
3.5 Example: Edge-Factored Model As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999).

Title:C10-1011		Conference:COLING		Author:Bohnet, Bernd
We combined this parsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al., 2003; McDonald et al., 2005; Crammer et al., 2006).

Title:W10-2927		Conference:Proceedings of the Fourteenth Conference on Computational Natural Language Learning		Author:Goldberg, Yoav; Elhadad, Michael
Parsers For graph-based parsers, we used the projective first-order (MST1) and secondorder (MST2) variants of the freely available MST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006).

Title:P10-1151		Conference:ACL		Author:G&oacute;mez-Rodr&iacute;guez, Carlos; Nivre, Joakim
1 Introduction Dependency-based syntactic parsing has become a widely used technique in natural language processing, and many different parsing models have been proposed in recent years (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Titov and Henderson, 2007; Martins et al., 2009).

Title:P10-1151		Conference:ACL		Author:G&oacute;mez-Rodr&iacute;guez, Carlos; Nivre, Joakim
Like context-free grammars, projective dependency trees are not sufficient to represent all the linguistic phenomena observed in natural languages, but they have the advantage of being efficiently parsable: their parsing problem can be solved in cubic time with chart parsing techniques (Eisner, 1996; Gomez-Rodrguez et al., 2008), while in the case of general non-projective dependency forests, it is only tractable under strong independence assumptions (McDonald et al., 2005b; McDonald and Satta, 2007).

Title:N10-1090		Conference:Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics		Author:Zhang, Yaozhong; Matsuzaki, Takuya; Tsujii, Jun'ichi
Two representative methods for dependency parsing are transitionbased model like MaltParser (Nivre, 2003) and graph-based model like MSTParser 1 (McDonald et al., 2005).

Title:P10-3010		Conference:Proceedings of the ACL 2010 Student Research Workshop		Author:Haulrich, Martin
For the MST-parsing MIRA (McDonald et al., 2005a; McDonald and Pereira, 2006) and for transition-based parsing Support-Vector Machines (Hall et al., 2006; Nivre et al., 2006b).

Title:P10-3010		Conference:Proceedings of the ACL 2010 Student Research Workshop		Author:Haulrich, Martin
The two dominating approaches have been graph-based parsing, e.g. MST-parsing (McDonald et al., 2005b) and transition-based parsing, e.g. the MaltParser (Nivre et al., 2006a).

Title:W10-1757		Conference:Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR		Author:Duh, Kevin; Sudoh, Katsuhito; Tsukada, Hajime; Isozaki, Hideki; Nagata, Masaaki
In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process.

Title:D10-1095		Conference:EMNLP		Author:Mejer, Avihai; Crammer, Koby
For example the passive-aggressive algorithm was adapted to chunking (Shimizu and Haas, 2006), parsing (McDonald et al., 2005b), learning preferences (Wick et al., 2009) and text segmentation (McDonald et al., 2005a). (self citation)

Title:N10-1093		Conference:Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics		Author:Gadde, Phani; Jindal, Karan; Husain, Samar; Sharma, Dipti Misra; Sangal, Rajeev
It uses online large margin learning as the learning algorithm (McDonald et al., 2005b).

Title:N10-1093		Conference:Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics		Author:Gadde, Phani; Jindal, Karan; Husain, Samar; Sharma, Dipti Misra; Sangal, Rajeev
All the experiments are done using a modified version of MSTParser (McDonald et al., 2005a and the references therein) (henceforth MST) on the ICON 2009 parsing contest2 (Husain, 2009) data.

Title:D10-1069		Conference:EMNLP		Author:Petrov, Slav; Chang, Pi-Chuan; Ringgaard, Michael; Alshawi, Hiyan
The table also confirms the commonly known fact (Yamada and Matsumoto, 2003; McDonald et al., 2005) that constituency parsers are more accurate at producing dependencies than dependency parsers (at least when the dependencies were produced by a deterministic transformation of a constituency treebank, as is the case here).

Title:P10-1002		Conference:ACL		Author:Jiang, Wenbin; Liu, Qun
5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model.

Title:P10-1002		Conference:ACL		Author:Jiang, Wenbin; Liu, Qun
1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006).

Title:P10-1002		Conference:ACL		Author:Jiang, Wenbin; Liu, Qun
The dependency probability can then be defined as: C(i,j) = exp(w f(i,j,+))summationtext r exp(w f(i,j,r)) = exp( summationtext k wk fk(i,j,+))summationtext r exp( summationtext k wk fk(i,j,r)) (5) 2.2 Features for Classification The feature templates for the classifier are similar to those of 1st-ordered MST model (McDonald et al., 2005a).

Title:P10-1002		Conference:ACL		Author:Jiang, Wenbin; Liu, Qun
2 Word-Pair Classification Model 2.1 Model Definition Following (McDonald et al., 2005a), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence.

Title:C10-1102		Conference:COLING		Author:Qian, Xian; Zhang, Qi; Huang, Xuangjing; Wu, Lide
Such as shallow parsing (Sha and Pereira, 2003), named entity recognition (Kazama and Torisawa, ), dependency parsing (McDonald et al., 2005), etc. The problem arises when the number of templates increases, more features generated, making the extraction step time consuming.

Title:C10-1094		Conference:COLING		Author:Nivre, Joakim; Rimell, Laura; McDonald, Ryan; G&oacute;mez-Rodr&iacute;guez, Carlos
1 Introduction Though syntactic parsers for English are reported to have accuracies over 90% on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (McDonald et al., 2005; Sagae and Lavie, 2006; Huang, 2008; Carreras et al., 2008), broad-coverage parsing is still far from being a solved problem. (self citation)

Title:W10-1403		Conference:Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages		Author:Ambati, Bharat Ram; Husain, Samar; Jain, Sambhav; Sharma, Dipti Misra; Sangal, Rajeev
It uses online large margin learning as the learning algorithm (McDonald et al., 2005).

Title:W10-1401		Conference:Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages		Author:Tsarfaty, Reut; Seddah, Djam&eacute; ; Goldberg, Yoav; K&uuml;bler, Sandra; Versley, Yannick; Candito, Marie; Foster, Jennifer; Rehbein, Ines; Tounsi, Lamia
Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009).

Title:W10-1401		Conference:Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages		Author:Tsarfaty, Reut; Seddah, Djam&eacute; ; Goldberg, Yoav; K&uuml;bler, Sandra; Versley, Yannick; Candito, Marie; Foster, Jennifer; Rehbein, Ines; Tounsi, Lamia
This was shown to be the case for both MaltParser (Nivre et al., 2007c) and MST (McDonald et al., 2005), two of the best performing parsers on the whole.

Title:C10-1007		Conference:COLING		Author:Bergsma, Shane; Cherry, Colin
We test on two stateof-the art parsers: MST We modified the publicly-available MST parser (McDonald et al., 2005)6 to employ our filters before carrying out feature extraction.

Title:C10-1007		Conference:COLING		Author:Bergsma, Shane; Cherry, Colin
Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al., 2005).

Title:N10-1115		Conference:Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics		Author:Goldberg, Yoav; Elhadad, Michael
Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., MALTPARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., MSTPARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach.

Title:C10-3009		Conference:COLING - Demos		Author:Bj&ouml;rkelund, Anders; Bohnet, Bernd; Hafdell, Love; Nugues, Pierre
The parser is trained with the margin infused relaxed algorithm (MIRA) (McDonald et al., 2005) and combined with a hash kernel (Shi et al., 2009).

Title:D11-1113		Conference:EMNLP		Author:Henestroza Anguiano, Enrique; Candito, Marie
3.4 Baseline Parsers In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al., 2006) for constituency-based parsing.

Title:D11-1113		Conference:EMNLP		Author:Henestroza Anguiano, Enrique; Candito, Marie
While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisners algorithm (Eisner, 1996) are O(n3) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010).

Title:P11-2033		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Zhang, Yue; Nivre, Joakim
189 In this short paper, we extend the baseline feature templates with the following: Distance between S0 and N0 Direction and distance between a pair of head and modifier have been used in the standard feature templates for maximum spanning tree parsing (McDonald et al., 2005).

Title:P11-2033		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Zhang, Yue; Nivre, Joakim
Compared to graph-based dependency parsing, it typically offers linear time complexity and the comparative freedom to define non-local features, as exemplified by the comparison between MaltParser and MSTParser (Nivre et al., 2006b; McDonald et al., 2005; McDonald and Nivre, 2007).

Title:P11-1160		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Mannem, Prashanth; Dara, Aswarth
A bidirectionalEnglish POS tagger (Shen et al., 2007) was used to POS tag the source sentencesand the parses were obtained using the first order MST parser (McDonald et al., 2005)trainedon dependenciesextracted from Penn treebank using the head rules of Yamadaand Matsumoto(2003).A CRFbasedHindi POS tagger (PVS.

Title:W11-1203		Conference:Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web		Author:Andrade, Daniel; Matsuzaki, Takuya; Tsujii, Jun'ichi
The English corpus NHTSA was POS-tagged and stemmed with Stepp Tagger (Tsuruoka et al., 2005; Okazaki et al., 2008) and dependency parsed using the MST parser (McDonald et al., 2005).

Title:P11-1144		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Das, Dipanjan; Smith, Noah A.
The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005) following Das et al.

Title:D11-1006		Conference:EMNLP		Author:McDonald, Ryan; Petrov, Slav; Hall, Keith
This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). (self citation)

Title:D11-1006		Conference:EMNLP		Author:McDonald, Ryan; Petrov, Slav; Hall, Keith
Preliminary experiments using a different dependency parser MSTParser (McDonald et al., 2005) resulted in similar empirical observations. (self citation)

Title:P11-2035		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Cherry, Colin; Bergsma, Shane
At test time, we extract subclassifiers from the joint weight vector, and use them as parameters in the filtering tools of Bergsma and Cherry (2010).6 Parsing experiments are carried out using the MST parser (McDonald et al., 2005),7 which we have modified to filter arcs before carrying out feature extraction.

Title:P11-2035		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Cherry, Colin; Bergsma, Shane
Potential arcs are scored using rich linear modelsthatarediscriminativelytrainedtomaximize parsing accuracy (McDonald et al., 2005).

Title:D11-1036		Conference:EMNLP		Author:Tsarfaty, Reut; Nivre, Joakim; Andersson, Evelina
Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a).

Title:P11-1089		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Lee, John; Naradowsky, Jason; Smith, David A.
Parsing is performed using the usual pipeline approach, first with the TreeTagger analyzer (Schmid, 1994) and then with a state-of-the-art dependency parser (McDonald et al., 2005).

Title:P11-1089		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Lee, John; Naradowsky, Jason; Smith, David A.
The differences may partially be attributed to the different morphological tagger used, and the different learning algorithm, namely Margin Infused Relaxed Algorithm (MIRA) in (McDonald et al., 2005) rather than maximum likelihood.

Title:P11-1089		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Lee, John; Naradowsky, Jason; Smith, David A.
In short, the features of the parser are a replication of (McDonald et al., 2005), but also extended beyond POS to the other morphological attributes, with the features in the MORPH-LINK factors incorporated into WORD-LINK for similar reasons.

Title:D11-1045		Conference:EMNLP		Author:Visweswariah, Karthik; Rajkumar, Rajakrishnan; Gandhe, Ankur; Ramanathan, Ananthakrishnan; Navratil, Jiri
We show that the TSP distances for reordering can be learned from a small amount of high-quality word alignment data by means of pairwise word comparisons and an informative feature set involving words and part-of-speech (POS) tags adapted and extended from prior work on dependency parsing (McDonald et al., 2005b).

Title:D11-1045		Conference:EMNLP		Author:Visweswariah, Karthik; Rajkumar, Rajakrishnan; Gandhe, Ankur; Ramanathan, Ananthakrishnan; Navratil, Jiri
the translation, distortion and language model probabilities) we learn model weights to reorder source sentences to match target word order using an informative feature set adapted from graph-based dependency parsing (McDonald et al., 2005a).

Title:D11-1045		Conference:EMNLP		Author:Visweswariah, Karthik; Rajkumar, Rajakrishnan; Gandhe, Ankur; Ramanathan, Ananthakrishnan; Navratil, Jiri
4.1 Features Sincewewouldliketomodelreorderingphenomena which are largely related to analyzing the syntax of the source sentence, we chose to use features based on those that have in the past been used for parsing (McDonald et al., 2005a).

Title:D11-1138		Conference:EMNLP		Author:Hall, Keith; McDonald, Ryan; Katz-Brown, Jason; Ringgaard, Michael
We will show in the next section that our augmentedloss method is general and can be applied to any dependency parsing framework that can be trained by the perceptron algorithm, such as transition-based parsers (Nivre, 2008; Zhang and Clark, 2008) and graph-based parsers (McDonald et al., 2005). (self citation)

Title:D11-1138		Conference:EMNLP		Author:Hall, Keith; McDonald, Ryan; Katz-Brown, Jason; Ringgaard, Michael
Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (McDonald et al., 2005). (self citation)

Title:P11-1069		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Zhang, Yue; Clark, Stephen
The comparison reported in this section is similar to the comparison between the chartbased MSTParser (McDonald et al., 2005) and shiftreduce MaltParser (Nivre et al., 2006) for dependency parsing.

Title:P11-1069		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Zhang, Yue; Clark, Stephen
Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald et al., 2005) and shift-reduce MaltParser (Nivre et al., 2006) for dependency parsing.

Title:P11-1070		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Bansal, Mohit; Klein, Dan
For the dependency case, we can integrate them into the dynamic programming of a base parser; we use the discriminativelytrained MST dependency parser (McDonald et al., 2005; McDonald and Pereira, 2006).

Title:W11-1402		Conference:Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications		Author:Xiong, Wenting; Litman, Diane J.
3We used MSTParser (McDonald et al., 2005) for syntactic analysis.

Title:P11-2125		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Haffari, Gholamreza; Razavi, Marzieh; Sarkar, Anoop
All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008).

Title:P11-2125		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Haffari, Gholamreza; Razavi, Marzieh; Sarkar, Anoop
The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)).

Title:P11-2125		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Haffari, Gholamreza; Razavi, Marzieh; Sarkar, Anoop
In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005).

Title:P11-2125		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Haffari, Gholamreza; Razavi, Marzieh; Sarkar, Anoop
c2011 Association for Computational Linguistics An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing Gholamreza Haffari Faculty of Information Technology Monash University Melbourne, Australia reza@monash.edu Marzieh Razavi and Anoop Sarkar School of Computing Science Simon Fraser University Vancouver, Canada {mrazavi,anoop}@cs.sfu.ca Abstract We combine multiple word representations based on semantic clusters extracted from the (Brown et al., 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al., 2006) in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005).

Title:P11-2125		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Haffari, Gholamreza; Razavi, Marzieh; Sarkar, Anoop
This paper makes two contributions: 1) We combine together multiple word representations based on semantic and syntactic clusters in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005), and 2) We provide an ensemble method for combining diverse clustering algorithms that is the discriminative parsing analog to the generative product of experts model for parsing described in (Petrov, 2010).

Title:D11-1137		Conference:EMNLP		Author:Hayashi, Katsuhiko; Watanabe, Taro; Asahara, Masayuki; Matsumoto, Yuji
(2005) third-order tri-sibling model (tri-sibling) Model 2 (Koo and Collins, 2010) third-order grandsibling model (Sangati et al., 2009) (grandsibling) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses firstand second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006).

Title:D11-1137		Conference:EMNLP		Author:Hayashi, Katsuhiko; Watanabe, Taro; Asahara, Masayuki; Matsumoto, Yuji
We obtain k-best listsandforestsgeneratedfromthebaselinediscriminative model which has the same feature set as provided in (McDonald et al., 2005), using the secondorder Eisner algorithms.

Title:P11-2088		Conference:Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies		Author:Xiong, Wenting; Litman, Diane J.
While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.1 1We used MSTParser (McDonald et al., 2005) for syntactic analysis.

Title:D11-1004		Conference:EMNLP		Author:Galley, Michel; Quirk, Chris
1 Introduction Minimum error rate training (MERT)also known as direct loss minimization in machine learningis a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010).

Title:D11-1017		Conference:EMNLP		Author:Katz-Brown, Jason; Petrov, Slav; McDonald, Ryan; Och, Franz Josef; Talbot, David; Ichikawa, Hiroshi; Seno, Masakazu; Kazawa, Hideto
1 Introduction The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). (self citation)

