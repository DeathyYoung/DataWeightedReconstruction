Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3
Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing
The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005)
4 Non-Projective Relations For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective
We used a pseudo-projective edge label encoding to handle nonprojectivity (Nivre and Nilsson, 2005)
All non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm of Nivre and Nilsson (2005)
Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice are close to being projective, since they contain only a small proportion of nonprojective arcs
20 Because the frequency of non-projective dependencies in the Turkish Treebank is not high enough to learn such dependencies and mostly due to the unconnected punctuations with which we are dealing by adding an extra dependency label,we did not observe any improvement when applying the pseudo-projective processing of Nivre and Nilsson (2005),which is reported to improve accuracy for other languages
In order to realize the full potential ofdependency-based syntactic parsing, it isdesirable to allow non-projective depen-dency structures
We show how a data-driven deterministic dependency parser,in itself restricted to projective structures,can be combined with graph transforma-tion techniques to produce non-projectivestructures
However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novk, 2005; McDonald and Pereira, 2006)
We will focus on tree transformations that combine preprocessing with post-processing, and where the parser is treated as a black box, such as the pseudo-projective parsing technique proposed by Nivre and Nilsson (2005) and the tree transformations investigated in Nilsson et al
1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al
Recently, dependency parsing has gained popularity as a simpler, computationally more efficient alternative to constituency parsing and has spurred several supervised learning approaches (Eisner, 1996; Yamada and Matsumoto, 2003a; Nivre and Nilsson, 2005; McDonald et al
You could map a non-projective dependency tree to a projective one, learn and predict the tree, then bring it back to the non-projective dependency tree (Nivre and Nilsson, 2005)
