We propose a semi-automatic tool, ter- might, that helps professional translators and terminologists identify technical terms and their translations. 
The tool makes use of part-of-speech tagging and word- alignment programs to extract candidate terms and their translations. 
Although the extraction programs are far from perfect, it isn't too hard for the user to filter out the wheat from the chaff. 
The extraction algorithms emphasize completeness. Alter- native proposals are likely to miss impor- tant but infrequent terms/translations.
To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes. 
Ter- might is currently being used by the trans-...
Several methods have been proposed with regard to acquiring various kinds of rules such as translation rules, grammar rules, dictionary entries and so on from bilingual corpora (Dagan et al. , 1991; Dagan and Church, 1994; Fung and Church, 1994; Tanaka, 1994; Yamada et al. , 1995).
(Justeson and Katz, 1995) and (Dagan and Church, 1994) use the frequency of occurrence of the candidate string as a measure of its likelihood to be a term.
However, in the experiments described here, we focus on alignment at the level of sentences, this for a number of reasons: First, sentence alignments have so far proven their usefulness in a number of applications, e.g. bilingual lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Dagan and Church, 1994), automatic translation verification (Macklovitch, 1995; Macklovitch, 1996) and the automatic acquisition of knowledge about translation (Brown et al. , 1993).
Most of the research has focused on bilingual terminology identification, either as parallel multiwords forms (e.g. the ChampoUion system (Smadja et a1.1996)), technical terminology (e.g. the Termight system (Dagan and Church, 1994) or broad-coverage translation lexicons (e.g. the SABLE system (Resnik and Melamed, 1997)).
NP chunks (Abney 1991; Ramshaw and Marcus 1995; Evans and Zhai 1996; Frantzi and Ananiadou 1996) and technical terms (Dagan and Church 1994; Justeson and Katz 1995; Daille 1996; Jacquemin 2001; Bourigault et al. 2002) fall into this difficult-toassess category.
2 The two techniques for automatic identification were the technical terms algorithm of Justeson and Katz (1995) and the head sorting method (Dagan and Church (1994); Wacholder (1998).
On the one hand, the linguistically-based or rule-based approaches use linguistic information such as PoS tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (Ananiadou, 1994), (Dagan and Church, 1994).
In fact, many automatic tools have been proposed to assist humans in this important task (Itagaki et al, 2007; Dagan and Church, 1994, among others)
A simpler, related idea of penalizing distortion from some ideal matching pattern can be found in the statistical translation (Brown et al. 1990; Brown et al. 1993) and word alignment (Dagan et al. 1993; Dagan & Church 1994) models.
