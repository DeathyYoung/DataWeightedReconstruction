Mihalcea and Pedersen (2003) described a shared task where the goal was to achieve the best AER.
While these models have proven effective at the word alignment task (Mihalcea & Pedersen 2003), there are significant practical limitations in their output.
2 Prior Work The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the wordalignment task, and established standard performance measures and some benchmark tasks.
3 Experiments We applied this matching algorithm to wordlevel alignment using the English-French Hansards data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003).
It was the basis for a system that performed very well in a comparison of several alignment systems (Dejean et al. , 2003; Mihalcea and Pedersen, 2003).
The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003).
The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS)1 annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003).
For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003).
The results achieved by the technique proposed in this paper are compared with the best results presented in the shared tasks described in (Mihalcea and Pedersen, 2003) (Joel Martin, 2005).
They indicate how close the alignment under investigation is to the gold standard alignment (Mihalcea and Pedersen, 2003).
To choose the regularization strength and the initial learning rate 0,3 we trained several models on a 10,000-sentence-pair subset of the FrenchEnglish Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003).
The French/English data were those used by Mihalcea and Pedersen (2003).
5 Data and Methodology for Evaluation We evaluated our models using data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003).
Mihalcea and Pedersen (2003) described a shared task where the goal was to achieve the best AER.
6 Conclusions We have developed a system to improve the performance of bitext word alignment between English and a source language by first reordering parsed English into an order more closely resembling that 1Hindi training: news text from the LDC for the 2003 DARPA TIDES Surprise Language exercise; Hindi testing: news text from Rebecca Hwa, then at the University of Maryland; Hindi dictionary: The Hindi-English Dictionary, v. 2.0 from IIIT (Hyderabad) LTRC; Korean training: Unbound Bible; Korean testing: half from Penn Korean Treebank and half from Universal declaration of Human Rights, aligned by Woosung Kim at the Johns Hopkins University; Korean dictionary: EngDic v. 4; Chinese training: news text from FBIS; Chinese testing: Penn Chinese Treebank news text aligned by Rebecca Hwa, then at the University of Maryland; Chinese dictionary: from the LDC; Romanian training and testing: (Mihalcea and Pedersen, 2003).
