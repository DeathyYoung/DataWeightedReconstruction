Ensemble methods like bagging and boosting that combine the decisions of multiple hypotheses are some of the strongest existing machine learning methods
The diversity of the members of an ensemble is known to be an important factor in determining its generalization error
This paper presents a new method for generating ensembles that directly constructs diverse hypotheses using additional artificially-constructed training exam- ples
The technique is a simple, general meta- learner that can use any strong learner as a base classifier to build diverse committees
Experimen- tal results using decision-tree induction as a base learner demonstrate that this approach consistently achieves higher predictive accuracy than both the base classifier and bagging (whereas boosting can occasionally decrease accuracy), and also obtains higher accuracy than boosting early in the learning curve when training data is limited
Using an ensemble of classifiers from DECORATE[15] and decision trees, they were able to improve the accuracy prediction of semantic ratings by 50 % on average despite the variability in the radiologists¡¯ interpretation
One of the recently proposed ensemble building techniques that could also be seen as a somehow alternative approach as it significantly differs from the above described techniques is called DECORATE (Diverse Ensemble Creation by Oppositional Relabeling of Artificial Training Examples) [16]
Decorate [9, 10] is a recently introduced ensemble meta-learner that directly constructs diverse committees by employing specially-constructed artificial 1 An ensemble meta-learner, like Bagging and AdaBoost, takes an arbitary base
Melville and Mooney [12] present a new meta-learner (DECORATE, Diverse Ensemble Creation by Oppositional Re-labeling of Artificial Training Examples) that uses an existing ¡°strong¡± learner (one that provides high accuracy on the training data) to build a diverse committee
classifiers in Decorate is provided by adding different randomly constructed examples to the training set when building new members of ensemble [15]
Since a diverse set of classifiers is essential for achieving improved accuracies [7, 10], by taking into account the improvements achieved using GeneticBoost, it can be argued that the diversity requirement is achieved up to some extent
The generalization accuracy of an ensemble depends on the diversity of the classifiers, which means that the classifiers in the ensemble should be different from each other, producing different errors on the input samples [6-11]
For instance, in their ensemble design approach named DECORATE, Melville and Mooney [10] minimized the ensemble error by increasing the diversity
Since the generalization accuracy of the ensemble also depends on the diversity of the classifiers, they should be different from each other, producing different errors on the input samples [6-11]
Although a number of studies [3][4][5][6] have been applied in machine learning and data mining domains in recent decades, a common problem is that new data that are generated from the whole problem domain maybe unuseful and also result in computationally intensive problem
