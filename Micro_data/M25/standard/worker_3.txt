A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described
The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification
Hindle (1990) clusters similar nouns by comparing their behaviour regarding the predicate argument structure
As a result, almost all extraction techniques suggest that a collocation must differ from other word combinations in some kind of frequency measure and discriminates between collocations based on frequency of word occurrences (Church & Hanks, 1990; Hindle, 1990; Dunning, 1993; Smadja, 1993; Bisht et al, 2006; etc
¡± (Hindle, 1990) Previous distributional similarity metrics, however, have been designed for comparing words based on terms appearing in the same document, rather than extracted properties
Examples of similarity measures are information content [26], mutual information [15], Dice coefficient [11], cosine coefficient [11], distance-based measurements [18], information theory-based [20] and contrast models [28]
Large-corpus methods for bootstrapping domain-specific lexicons have existed for quite some time, as have methods for partitioning words into pseudo-semantic classes based on their distributional properties (Church and Hanks, 1990; Hearst, 1992; Hindle, 1990; Pereira et al
Hindle [7], for example, extracted verb-noun relationships of subjects/objects and their predicates from a corpus and proposed a method to calculate similarity of two words based on their mutual information
Under the corpus-based approach, word relationships are often derived from their co-occurrence distribution in a corpus (Church and Hanks 1989, Hindle 1990, Grefenstette 1992)
The work reported here attempts to find semantic similarity among terms based on the contexts they tend to occur in; (Church & Hanks 1990) uses frequency of co-occurrence of content words to create clusters of semantically similar words, (Hindle 1990) uses both simple syntactic subject-verb-object frames and frequency of occurrence of content words to determine similarity among nouns, and (Calzolari & Bindi 1990) use corpus-based
Hindle [14] proposed an automatic method based on the representation of terms through frequent syntactic features, and thus he determined similarity between nouns
Under the corpus-based approach, word relationships are often derived from their co-occurrence distribution in a corpus (Church and Hanks 1989, Hindle 1990, Grefenstette 1992)
(Hindle, 1990), (Lin, 1998), (Grefenstette, 1994a), (Grefenstette, 1994b) all find and count triples of grammatical-relation, word1, word2 rather than simple unordered word co-occurrences
Usually, a vector-space model is used as input and the linguistic context of a term is described by, e
There are many of methods used to measure the similarity between different entities, such as information content [6], Dice coefficient [2], mutual information [3], cosine coefficient [2], distance-based measurements [4], and feature contrast model [8]
