A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described
The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification
Many similarity measures were developed, such as information content [21], mutual information [8], Dice coefficient [5], cosine coefficient [5], distance-based measurements [10,16], and feature contrast model [23]
Hindle¡¯s approach [7] considers lexical relationship between a verb and the head nouns of its subject and object
One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998)
Classes are produced by clustering techniques based on similar word contexts ¨C which describe words that are likely to be found in the immediate vicinity of a given word (Church and Hanks, 1990; Smadja, 1993) ¨C or on similar distributional contexts ¨C which reveal words that share the same syntactic environments (Hindle, 1990; Grefenstette, 1994)
There has been some work in automatic clustering or discovery of similar words [8-12], but few of them have addressed term-level similarities and discussed their potential applications in ontology development
There are two different method to extract thesaural relationships from corpora predicate-argument (also called head-modifier) method [6, 5, 8, 7, 13] and co-occurrence statistical method [1, 2, 12, 15]
Hindle (1990) proposed a method dealing with the data-sparseness problem of low-frequency words that estimates the likelihood of unseen events from that of "similar" events that have been seen before
Under the assumption that instance and class nouns are likely to co-occur with the same verbs, we compute a similarity score between T and each noun Ni, by summing the product of the mutual information values for those verbs occurring with both nouns
Hindle [5] uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns, and claims this metric reflects semantic relatedness
A common approach (Philips, 1985; Hindle, 1990) is to represent the context a word appears in by the words occurring in that context, weighting more heavily the context elements that cooccur more often than expected for random cooccurrences
In a comparative study by Cimiano [6], conditional probability is reported to outperform other measures, such as pointwise mutual information (PMI) [20], and selectional strength [21]
Large-corpus methods for bootstrapping domain-specific lexicons have existed for quite some time, as have methods for partitioning words into pseudo-semantic classes based on their distributional properties (Church and Hanks, 1990; Hearst, 1992; Hindle, 1990; Pereira et al
Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools (Hindle, 1990), (Zernik, 1990), (Resnik, 1993), or for automatic thesaurus generation (Grefenstette, 1994)
